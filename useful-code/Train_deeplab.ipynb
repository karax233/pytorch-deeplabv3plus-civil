{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3932,"status":"ok","timestamp":1656722791740,"user":{"displayName":"党紀","userId":"14304698389961902712"},"user_tz":-540},"id":"eo6dm8P5T2if","outputId":"676b4a3c-05db-496f-bbfe-5f001efb25a6"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting tensorboardX\n","  Downloading tensorboardX-2.5.1-py2.py3-none-any.whl (125 kB)\n","\u001b[K     |████████████████████████████████| 125 kB 14.0 MB/s \n","\u001b[?25hRequirement already satisfied: protobuf\u003c=3.20.1,\u003e=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (3.17.3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (1.21.6)\n","Requirement already satisfied: six\u003e=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf\u003c=3.20.1,\u003e=3.8.0-\u003etensorboardX) (1.15.0)\n","Installing collected packages: tensorboardX\n","Successfully installed tensorboardX-2.5.1\n"]}],"source":["!pip install tensorboardX"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":967,"status":"ok","timestamp":1656722792686,"user":{"displayName":"党紀","userId":"14304698389961902712"},"user_tz":-540},"id":"epxFJ3qgQ_jg"},"outputs":[],"source":["import os\n","os.chdir('/content/drive/MyDrive/Lab2020/Liu/Deeplab_pytorch/pytorch-deeplab-xception')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"a89A8ijLQ6be"},"outputs":[{"name":"stdout","output_type":"stream","text":["Namespace(backbone='resnet', base_size=513, batch_size=6, checkname='train_corrosion_no_ft_04', crop_size=513, cuda=True, dataset='corrosion', epochs=100, eval_interval=1, freeze_bn=False, ft=False, gpu_ids=[0], loss_type='ce', lr=0.01, lr_scheduler='poly', momentum=0.9, nesterov=False, no_cuda=False, no_val=False, out_stride=16, resume=None, seed=1, start_epoch=0, sync_bn=False, test_batch_size=6, use_balanced_weights=False, use_sbd=True, weight_decay=0.0005, workers=1)\n","Number of images in train: 1592\n","Number of images in val: 399\n","Using poly LR Scheduler!\n","Starting Epoch: 0\n","Total Epoches: 100\n","  0% 0/266 [00:00\u003c?, ?it/s]\n","=\u003eEpoches 0, learning rate = 0.0100,                 previous best = 0.0000\n","/usr/local/lib/python3.7/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n","  warnings.warn(warning.format(ret))\n","Train loss: 0.066: 100% 266/266 [23:45\u003c00:00,  5.36s/it]\n","[Epoch: 0, numImages:  1592]\n","Loss: 17.596\n","Test loss: 0.059: 100% 67/67 [05:39\u003c00:00,  5.07s/it]\n","Validation:\n","[Epoch: 0, numImages:   399]\n","Acc:0.8641919787175457, Acc_class:0.6880272557174079, mIoU:0.5839950107889791, fwIoU: 0.7797132239858362\n","Loss: 3.980\n","  0% 0/266 [00:00\u003c?, ?it/s]\n","=\u003eEpoches 1, learning rate = 0.0099,                 previous best = 0.5840\n","Train loss: 0.057: 100% 266/266 [01:39\u003c00:00,  2.67it/s]\n","[Epoch: 1, numImages:  1592]\n","Loss: 15.266\n","Test loss: 0.068: 100% 67/67 [00:17\u003c00:00,  3.87it/s]\n","Validation:\n","[Epoch: 1, numImages:   399]\n","Acc:0.8395649036943974, Acc_class:0.6933125775233718, mIoU:0.5632141478192394, fwIoU: 0.7541248139590843\n","Loss: 4.589\n","  0% 0/266 [00:00\u003c?, ?it/s]\n","=\u003eEpoches 2, learning rate = 0.0098,                 previous best = 0.5840\n","Train loss: 0.056: 100% 266/266 [01:39\u003c00:00,  2.68it/s]\n","[Epoch: 2, numImages:  1592]\n","Loss: 14.800\n","Test loss: 0.053: 100% 67/67 [00:17\u003c00:00,  3.86it/s]\n","Validation:\n","[Epoch: 2, numImages:   399]\n","Acc:0.8847295025102322, Acc_class:0.7161882734849884, mIoU:0.6223402999961949, fwIoU: 0.8056732164440944\n","Loss: 3.530\n","  0% 0/266 [00:00\u003c?, ?it/s]\n","=\u003eEpoches 3, learning rate = 0.0097,                 previous best = 0.6223\n","Train loss: 0.053: 100% 266/266 [01:40\u003c00:00,  2.66it/s]\n","[Epoch: 3, numImages:  1592]\n","Loss: 14.064\n","Test loss: 0.050: 100% 67/67 [00:17\u003c00:00,  3.79it/s]\n","Validation:\n","[Epoch: 3, numImages:   399]\n","Acc:0.894998926283406, Acc_class:0.6871596098039984, mIoU:0.6174454588517537, fwIoU: 0.8132534476619854\n","Loss: 3.375\n","  0% 0/266 [00:00\u003c?, ?it/s]\n","=\u003eEpoches 4, learning rate = 0.0096,                 previous best = 0.6223\n","Train loss: 0.051: 100% 266/266 [01:39\u003c00:00,  2.66it/s]\n","[Epoch: 4, numImages:  1592]\n","Loss: 13.626\n","Test loss: 0.049: 100% 67/67 [00:17\u003c00:00,  3.81it/s]\n","Validation:\n","[Epoch: 4, numImages:   399]\n","Acc:0.8925630481250834, Acc_class:0.6395892817044693, mIoU:0.5802622344268193, fwIoU: 0.802349649229273\n","Loss: 3.273\n","  0% 0/266 [00:00\u003c?, ?it/s]\n","=\u003eEpoches 5, learning rate = 0.0095,                 previous best = 0.6223\n","Train loss: 0.051: 100% 266/266 [01:39\u003c00:00,  2.67it/s]\n","[Epoch: 5, numImages:  1592]\n","Loss: 13.564\n","Test loss: 0.053: 100% 67/67 [00:17\u003c00:00,  3.82it/s]\n","Validation:\n","[Epoch: 5, numImages:   399]\n","Acc:0.8772742171232755, Acc_class:0.591573610958747, mIoU:0.5277271724078003, fwIoU: 0.777459771679663\n","Loss: 3.543\n","  0% 0/266 [00:00\u003c?, ?it/s]\n","=\u003eEpoches 6, learning rate = 0.0095,                 previous best = 0.6223\n","Train loss: 0.051: 100% 266/266 [01:40\u003c00:00,  2.63it/s]\n","[Epoch: 6, numImages:  1592]\n","Loss: 13.561\n","Test loss: 0.052: 100% 67/67 [00:17\u003c00:00,  3.84it/s]\n","Validation:\n","[Epoch: 6, numImages:   399]\n","Acc:0.8789705741084393, Acc_class:0.7330716811936444, mIoU:0.6242316512557009, fwIoU: 0.8010659683237383\n","Loss: 3.510\n","  0% 0/266 [00:00\u003c?, ?it/s]\n","=\u003eEpoches 7, learning rate = 0.0094,                 previous best = 0.6242\n","Train loss: 0.051: 100% 266/266 [01:44\u003c00:00,  2.55it/s]\n","[Epoch: 7, numImages:  1592]\n","Loss: 13.565\n","Test loss: 0.052: 100% 67/67 [00:17\u003c00:00,  3.77it/s]\n","Validation:\n","[Epoch: 7, numImages:   399]\n","Acc:0.8856957664957967, Acc_class:0.6158410021885857, mIoU:0.5546309125838328, fwIoU: 0.7906332959046419\n","Loss: 3.457\n","  0% 0/266 [00:00\u003c?, ?it/s]\n","=\u003eEpoches 8, learning rate = 0.0093,                 previous best = 0.6242\n","Train loss: 0.049: 100% 266/266 [01:41\u003c00:00,  2.61it/s]\n","[Epoch: 8, numImages:  1592]\n","Loss: 12.951\n","Test loss: 0.047: 100% 67/67 [00:17\u003c00:00,  3.78it/s]\n","Validation:\n","[Epoch: 8, numImages:   399]\n","Acc:0.8989068089898035, Acc_class:0.6896673491556404, mIoU:0.6239887827578726, fwIoU: 0.8180546426545722\n","Loss: 3.118\n","  0% 0/266 [00:00\u003c?, ?it/s]\n","=\u003eEpoches 9, learning rate = 0.0092,                 previous best = 0.6242\n","Train loss: 0.048: 100% 266/266 [01:44\u003c00:00,  2.55it/s]\n","[Epoch: 9, numImages:  1592]\n","Loss: 12.783\n","Test loss: 0.047: 100% 67/67 [00:18\u003c00:00,  3.72it/s]\n","Validation:\n","[Epoch: 9, numImages:   399]\n","Acc:0.9034826825546057, Acc_class:0.7133521434558554, mIoU:0.6452883252493983, fwIoU: 0.826932437892895\n","Loss: 3.127\n","  0% 0/266 [00:00\u003c?, ?it/s]\n","=\u003eEpoches 10, learning rate = 0.0091,                 previous best = 0.6453\n","Train loss: 0.048: 100% 266/266 [01:43\u003c00:00,  2.58it/s]\n","[Epoch: 10, numImages:  1592]\n","Loss: 12.825\n","Test loss: 0.047: 100% 67/67 [00:18\u003c00:00,  3.60it/s]\n","Validation:\n","[Epoch: 10, numImages:   399]\n","Acc:0.8988818005213514, Acc_class:0.7525825141038395, mIoU:0.6606825718216656, fwIoU: 0.8265043226460925\n","Loss: 3.159\n","  0% 0/266 [00:00\u003c?, ?it/s]\n","=\u003eEpoches 11, learning rate = 0.0090,                 previous best = 0.6607\n","Train loss: 0.047: 100% 266/266 [01:43\u003c00:00,  2.56it/s]\n","[Epoch: 11, numImages:  1592]\n","Loss: 12.417\n","Test loss: 0.046: 100% 67/67 [00:17\u003c00:00,  3.74it/s]\n","Validation:\n","[Epoch: 11, numImages:   399]\n","Acc:0.89912573308454, Acc_class:0.7246200693026911, mIoU:0.645868615342363, fwIoU: 0.823358687482574\n","Loss: 3.065\n","  0% 0/266 [00:00\u003c?, ?it/s]\n","=\u003eEpoches 12, learning rate = 0.0089,                 previous best = 0.6607\n","Train loss: 0.048: 100% 266/266 [01:42\u003c00:00,  2.59it/s]\n","[Epoch: 12, numImages:  1592]\n","Loss: 12.711\n","Test loss: 0.048: 100% 67/67 [00:17\u003c00:00,  3.78it/s]\n","Validation:\n","[Epoch: 12, numImages:   399]\n","Acc:0.8933075309936206, Acc_class:0.7010937231416384, mIoU:0.6241909539357835, fwIoU: 0.8134096600313038\n","Loss: 3.236\n","  0% 0/266 [00:00\u003c?, ?it/s]\n","=\u003eEpoches 13, learning rate = 0.0088,                 previous best = 0.6607\n","Train loss: 0.047: 100% 266/266 [01:39\u003c00:00,  2.67it/s]\n","[Epoch: 13, numImages:  1592]\n","Loss: 12.494\n","Test loss: 0.049: 100% 67/67 [00:17\u003c00:00,  3.76it/s]\n","Validation:\n","[Epoch: 13, numImages:   399]\n","Acc:0.8953296837540122, Acc_class:0.7756925608978424, mIoU:0.6667569913804204, fwIoU: 0.8246729718286075\n","Loss: 3.296\n","  0% 0/266 [00:00\u003c?, ?it/s]\n","=\u003eEpoches 14, learning rate = 0.0087,                 previous best = 0.6668\n","Train loss: 0.046: 100% 266/266 [01:42\u003c00:00,  2.60it/s]\n","[Epoch: 14, numImages:  1592]\n","Loss: 12.328\n","Test loss: 0.044: 100% 67/67 [00:17\u003c00:00,  3.79it/s]\n","Validation:\n","[Epoch: 14, numImages:   399]\n","Acc:0.9054205245871958, Acc_class:0.7402485620818344, mIoU:0.6638166427063295, fwIoU: 0.832912477446934\n","Loss: 2.962\n","  0% 0/266 [00:00\u003c?, ?it/s]\n","=\u003eEpoches 15, learning rate = 0.0086,                 previous best = 0.6668\n","Train loss: 0.045: 100% 266/266 [01:42\u003c00:00,  2.59it/s]\n","[Epoch: 15, numImages:  1592]\n","Loss: 11.982\n","Test loss: 0.048: 100% 67/67 [00:17\u003c00:00,  3.74it/s]\n","Validation:\n","[Epoch: 15, numImages:   399]\n","Acc:0.90236176795244, Acc_class:0.7554476772735172, mIoU:0.6673048950030127, fwIoU: 0.8310443023164276\n","Loss: 3.204\n","  0% 0/266 [00:00\u003c?, ?it/s]\n","=\u003eEpoches 16, learning rate = 0.0085,                 previous best = 0.6673\n","Train loss: 0.046: 100% 266/266 [01:45\u003c00:00,  2.52it/s]\n","[Epoch: 16, numImages:  1592]\n","Loss: 12.262\n","Test loss: 0.044: 100% 67/67 [00:18\u003c00:00,  3.56it/s]\n","Validation:\n","[Epoch: 16, numImages:   399]\n","Acc:0.9071414233938375, Acc_class:0.7430828431230991, mIoU:0.6680102774533965, fwIoU: 0.8353697471307243\n","Loss: 2.939\n","  0% 0/266 [00:00\u003c?, ?it/s]\n","=\u003eEpoches 17, learning rate = 0.0085,                 previous best = 0.6680\n","Train loss: 0.045: 100% 266/266 [01:42\u003c00:00,  2.59it/s]\n","[Epoch: 17, numImages:  1592]\n","Loss: 12.098\n","Test loss: 0.044: 100% 67/67 [00:18\u003c00:00,  3.53it/s]\n","Validation:\n","[Epoch: 17, numImages:   399]\n","Acc:0.9078557742006145, Acc_class:0.7151727926339989, mIoU:0.6527326996978525, fwIoU: 0.8323896760844214\n","Loss: 2.950\n","  0% 0/266 [00:00\u003c?, ?it/s]\n","=\u003eEpoches 18, learning rate = 0.0084,                 previous best = 0.6680\n","Train loss: 0.045: 100% 266/266 [01:42\u003c00:00,  2.61it/s]\n","[Epoch: 18, numImages:  1592]\n","Loss: 11.954\n","Test loss: 0.047: 100% 67/67 [00:17\u003c00:00,  3.83it/s]\n","Validation:\n","[Epoch: 18, numImages:   399]\n","Acc:0.9052765497105546, Acc_class:0.703100388223965, mIoU:0.6413383667935382, fwIoU: 0.827504023412984\n","Loss: 3.155\n","  0% 0/266 [00:00\u003c?, ?it/s]\n","=\u003eEpoches 19, learning rate = 0.0083,                 previous best = 0.6680\n","Train loss: 0.045: 100% 266/266 [01:41\u003c00:00,  2.62it/s]\n","[Epoch: 19, numImages:  1592]\n","Loss: 11.977\n","Test loss: 0.043: 100% 67/67 [00:17\u003c00:00,  3.79it/s]\n","Validation:\n","[Epoch: 19, numImages:   399]\n","Acc:0.9078322323369382, Acc_class:0.7474677703877255, mIoU:0.6714801777595916, fwIoU: 0.8367706885257463\n","Loss: 2.856\n","  0% 0/266 [00:00\u003c?, ?it/s]\n","=\u003eEpoches 20, learning rate = 0.0082,                 previous best = 0.6715\n","Train loss: 0.043: 100% 266/266 [01:42\u003c00:00,  2.58it/s]\n","[Epoch: 20, numImages:  1592]\n","Loss: 11.521\n","Test loss: 0.043: 100% 67/67 [00:17\u003c00:00,  3.84it/s]\n","Validation:\n","[Epoch: 20, numImages:   399]\n","Acc:0.9062825453527766, Acc_class:0.7601669787970378, mIoU:0.6757462582088342, fwIoU: 0.8363949166588298\n","Loss: 2.874\n","  0% 0/266 [00:00\u003c?, ?it/s]\n","=\u003eEpoches 21, learning rate = 0.0081,                 previous best = 0.6757\n","Train loss: 0.041: 100% 266/266 [01:43\u003c00:00,  2.57it/s]\n","[Epoch: 21, numImages:  1592]\n","Loss: 11.032\n","Test loss: 0.043: 100% 67/67 [00:17\u003c00:00,  3.85it/s]\n","Validation:\n","[Epoch: 21, numImages:   399]\n","Acc:0.9072632944413556, Acc_class:0.7748763374392768, mIoU:0.6846260170030406, fwIoU: 0.83926346939371\n","Loss: 2.897\n","  0% 0/266 [00:00\u003c?, ?it/s]\n","=\u003eEpoches 22, learning rate = 0.0080,                 previous best = 0.6846\n","Train loss: 0.044: 100% 266/266 [01:42\u003c00:00,  2.59it/s]\n","[Epoch: 22, numImages:  1592]\n","Loss: 11.728\n","Test loss: 0.044: 100% 67/67 [00:18\u003c00:00,  3.58it/s]\n","Validation:\n","[Epoch: 22, numImages:   399]\n","Acc:0.9050976334512969, Acc_class:0.7055940037519062, mIoU:0.6426957453966453, fwIoU: 0.8276780556581518\n","Loss: 2.958\n","  0% 0/266 [00:00\u003c?, ?it/s]\n","=\u003eEpoches 23, learning rate = 0.0079,                 previous best = 0.6846\n","Train loss: 0.044: 100% 266/266 [01:39\u003c00:00,  2.68it/s]\n","[Epoch: 23, numImages:  1592]\n","Loss: 11.737\n","Test loss: 0.043: 100% 67/67 [00:18\u003c00:00,  3.63it/s]\n","Validation:\n","[Epoch: 23, numImages:   399]\n","Acc:0.9093842906496012, Acc_class:0.7601011140941987, mIoU:0.6806404520931434, fwIoU: 0.840231833301728\n","Loss: 2.864\n","  0% 0/266 [00:00\u003c?, ?it/s]\n","=\u003eEpoches 24, learning rate = 0.0078,                 previous best = 0.6846\n","Train loss: 0.045: 100% 266/266 [01:40\u003c00:00,  2.66it/s]\n","[Epoch: 24, numImages:  1592]\n","Loss: 11.891\n","Test loss: 0.041: 100% 67/67 [00:18\u003c00:00,  3.55it/s]\n","Validation:\n","[Epoch: 24, numImages:   399]\n","Acc:0.9117175826608689, Acc_class:0.7688136266313234, mIoU:0.6889105435684391, fwIoU: 0.8441755525892974\n","Loss: 2.775\n","  0% 0/266 [00:00\u003c?, ?it/s]\n","=\u003eEpoches 25, learning rate = 0.0077,                 previous best = 0.6889\n","Train loss: 0.043: 100% 266/266 [01:41\u003c00:00,  2.62it/s]\n","[Epoch: 25, numImages:  1592]\n","Loss: 11.331\n","Test loss: 0.043: 100% 67/67 [00:18\u003c00:00,  3.58it/s]\n","Validation:\n","[Epoch: 25, numImages:   399]\n","Acc:0.9086358460434875, Acc_class:0.802333996952469, mIoU:0.6995048809941524, fwIoU: 0.8437857932604622\n","Loss: 2.876\n","  0% 0/266 [00:00\u003c?, ?it/s]\n","=\u003eEpoches 26, learning rate = 0.0076,                 previous best = 0.6995\n","Train loss: 0.043: 100% 266/266 [01:42\u003c00:00,  2.60it/s]\n","[Epoch: 26, numImages:  1592]\n","Loss: 11.429\n","Test loss: 0.042: 100% 67/67 [00:17\u003c00:00,  3.78it/s]\n","Validation:\n","[Epoch: 26, numImages:   399]\n","Acc:0.9117732469785013, Acc_class:0.7516510630862534, mIoU:0.6800385623968201, fwIoU: 0.8421751512206027\n","Loss: 2.809\n","  0% 0/266 [00:00\u003c?, ?it/s]\n","=\u003eEpoches 27, learning rate = 0.0075,                 previous best = 0.6995\n","Train loss: 0.042: 100% 266/266 [01:40\u003c00:00,  2.66it/s]\n","[Epoch: 27, numImages:  1592]\n","Loss: 11.228\n","Test loss: 0.040: 100% 67/67 [00:18\u003c00:00,  3.69it/s]\n","Validation:\n","[Epoch: 27, numImages:   399]\n","Acc:0.9142127630785409, Acc_class:0.7657215647287814, mIoU:0.6915270475674562, fwIoU: 0.8469699596640597\n","Loss: 2.709\n","  0% 0/266 [00:00\u003c?, ?it/s]\n","=\u003eEpoches 28, learning rate = 0.0074,                 previous best = 0.6995\n","Train loss: 0.042: 100% 266/266 [01:39\u003c00:00,  2.67it/s]\n","[Epoch: 28, numImages:  1592]\n","Loss: 11.045\n","Test loss: 0.045: 100% 67/67 [00:18\u003c00:00,  3.60it/s]\n","Validation:\n","[Epoch: 28, numImages:   399]\n","Acc:0.9041544637292497, Acc_class:0.7487763926907702, mIoU:0.6665256551600282, fwIoU: 0.8324357974402026\n","Loss: 2.982\n","  0% 0/266 [00:00\u003c?, ?it/s]\n","=\u003eEpoches 29, learning rate = 0.0073,                 previous best = 0.6995\n","Train loss: 0.041: 100% 266/266 [01:40\u003c00:00,  2.66it/s]\n","[Epoch: 29, numImages:  1592]\n","Loss: 10.971\n","Test loss: 0.041: 100% 67/67 [00:18\u003c00:00,  3.53it/s]\n","Validation:\n","[Epoch: 29, numImages:   399]\n","Acc:0.9123344518670836, Acc_class:0.7963610571305343, mIoU:0.7030943773274594, fwIoU: 0.847927822429613\n","Loss: 2.762\n","  0% 0/266 [00:00\u003c?, ?it/s]\n","=\u003eEpoches 30, learning rate = 0.0073,                 previous best = 0.7031\n","Train loss: 0.041: 100% 266/266 [01:42\u003c00:00,  2.60it/s]\n","[Epoch: 30, numImages:  1592]\n","Loss: 11.007\n","Test loss: 0.041: 100% 67/67 [00:18\u003c00:00,  3.65it/s]\n","Validation:\n","[Epoch: 30, numImages:   399]\n","Acc:0.9122428843026634, Acc_class:0.7507931883798109, mIoU:0.680340013343425, fwIoU: 0.8426526485497541\n","Loss: 2.779\n","  0% 0/266 [00:00\u003c?, ?it/s]\n","=\u003eEpoches 31, learning rate = 0.0072,                 previous best = 0.7031\n","Train loss: 0.040: 100% 266/266 [01:40\u003c00:00,  2.65it/s]\n","[Epoch: 31, numImages:  1592]\n","Loss: 10.704\n","Test loss: 0.041: 100% 67/67 [00:18\u003c00:00,  3.58it/s]\n","Validation:\n","[Epoch: 31, numImages:   399]\n","Acc:0.9145389588368895, Acc_class:0.7851183948780153, mIoU:0.7016816761621187, fwIoU: 0.8495868729266571\n","Loss: 2.770\n","  0% 0/266 [00:00\u003c?, ?it/s]\n","=\u003eEpoches 32, learning rate = 0.0071,                 previous best = 0.7031\n","Train loss: 0.042: 100% 266/266 [01:40\u003c00:00,  2.65it/s]\n","[Epoch: 32, numImages:  1592]\n","Loss: 11.053\n","Test loss: 0.044: 100% 67/67 [00:18\u003c00:00,  3.60it/s]\n","Validation:\n","[Epoch: 32, numImages:   399]\n","Acc:0.904050715726463, Acc_class:0.7965336127922855, mIoU:0.6894878469584945, fwIoU: 0.8374433529189076\n","Loss: 2.919\n","  0% 0/266 [00:00\u003c?, ?it/s]\n","=\u003eEpoches 33, learning rate = 0.0070,                 previous best = 0.7031\n","Train loss: 0.041: 100% 266/266 [01:40\u003c00:00,  2.64it/s]\n","[Epoch: 33, numImages:  1592]\n","Loss: 10.961\n","Test loss: 0.042: 100% 67/67 [00:18\u003c00:00,  3.62it/s]\n","Validation:\n","[Epoch: 33, numImages:   399]\n","Acc:0.9111900144480569, Acc_class:0.7673724286977068, mIoU:0.6873086997602345, fwIoU: 0.8433444733102525\n","Loss: 2.794\n","  0% 0/266 [00:00\u003c?, ?it/s]\n","=\u003eEpoches 34, learning rate = 0.0069,                 previous best = 0.7031\n","Train loss: 0.040: 100% 266/266 [01:40\u003c00:00,  2.64it/s]\n","[Epoch: 34, numImages:  1592]\n","Loss: 10.633\n","Test loss: 0.041: 100% 67/67 [00:17\u003c00:00,  3.78it/s]\n","Validation:\n","[Epoch: 34, numImages:   399]\n","Acc:0.9127345587920952, Acc_class:0.7568547819391598, mIoU:0.6844103218597378, fwIoU: 0.8440279274091809\n","Loss: 2.724\n","  0% 0/266 [00:00\u003c?, ?it/s]\n","=\u003eEpoches 35, learning rate = 0.0068,                 previous best = 0.7031\n","Train loss: 0.042: 100% 266/266 [01:42\u003c00:00,  2.60it/s]\n","[Epoch: 35, numImages:  1592]\n","Loss: 11.163\n","Test loss: 0.042: 100% 67/67 [00:17\u003c00:00,  3.78it/s]\n","Validation:\n","[Epoch: 35, numImages:   399]\n","Acc:0.9113407890377502, Acc_class:0.7179271607933664, mIoU:0.6597067985182454, fwIoU: 0.8369959458132926\n","Loss: 2.805\n","  0% 0/266 [00:00\u003c?, ?it/s]\n","=\u003eEpoches 36, learning rate = 0.0067,                 previous best = 0.7031\n","Train loss: 0.042: 100% 266/266 [01:41\u003c00:00,  2.62it/s]\n","[Epoch: 36, numImages:  1592]\n","Loss: 11.042\n","Test loss: 0.043: 100% 67/67 [00:17\u003c00:00,  3.80it/s]\n","Validation:\n","[Epoch: 36, numImages:   399]\n","Acc:0.9003239491864872, Acc_class:0.8088568384387085, mIoU:0.688884961005644, fwIoU: 0.8338996581774956\n","Loss: 2.869\n","  0% 0/266 [00:00\u003c?, ?it/s]\n","=\u003eEpoches 37, learning rate = 0.0066,                 previous best = 0.7031\n","Train loss: 0.039: 100% 266/266 [01:41\u003c00:00,  2.63it/s]\n","[Epoch: 37, numImages:  1592]\n","Loss: 10.486\n","Test loss: 0.041: 100% 67/67 [00:17\u003c00:00,  3.80it/s]\n","Validation:\n","[Epoch: 37, numImages:   399]\n","Acc:0.9139376604021596, Acc_class:0.7701630175163473, mIoU:0.693323389136172, fwIoU: 0.8471430599155149\n","Loss: 2.729\n","  0% 0/266 [00:00\u003c?, ?it/s]\n","=\u003eEpoches 38, learning rate = 0.0065,                 previous best = 0.7031\n","Train loss: 0.040: 100% 266/266 [01:42\u003c00:00,  2.61it/s]\n","[Epoch: 38, numImages:  1592]\n","Loss: 10.593\n","Test loss: 0.040: 100% 67/67 [00:17\u003c00:00,  3.81it/s]\n","Validation:\n","[Epoch: 38, numImages:   399]\n","Acc:0.9110984849772673, Acc_class:0.8014434740235297, mIoU:0.7032553700063635, fwIoU: 0.8468451877585484\n","Loss: 2.675\n","  0% 0/266 [00:00\u003c?, ?it/s]\n","=\u003eEpoches 39, learning rate = 0.0064,                 previous best = 0.7033\n","Train loss: 0.041: 100% 266/266 [01:42\u003c00:00,  2.59it/s]\n","[Epoch: 39, numImages:  1592]\n","Loss: 10.858\n","Test loss: 0.041: 100% 67/67 [00:17\u003c00:00,  3.80it/s]\n","Validation:\n","[Epoch: 39, numImages:   399]\n","Acc:0.9104695305667625, Acc_class:0.793657412067132, mIoU:0.6987065800343668, fwIoU: 0.8452715584711289\n","Loss: 2.742\n","  0% 0/266 [00:00\u003c?, ?it/s]\n","=\u003eEpoches 40, learning rate = 0.0063,                 previous best = 0.7033\n","Train loss: 0.040: 100% 266/266 [01:41\u003c00:00,  2.62it/s]\n","[Epoch: 40, numImages:  1592]\n","Loss: 10.614\n","Test loss: 0.041: 100% 67/67 [00:17\u003c00:00,  3.81it/s]\n","Validation:\n","[Epoch: 40, numImages:   399]\n","Acc:0.9143829654198117, Acc_class:0.7536649808499964, mIoU:0.6854456079726604, fwIoU: 0.8457018424261117\n","Loss: 2.772\n","  0% 0/266 [00:00\u003c?, ?it/s]\n","=\u003eEpoches 41, learning rate = 0.0062,                 previous best = 0.7033\n","Train loss: 0.040: 100% 266/266 [01:40\u003c00:00,  2.64it/s]\n","[Epoch: 41, numImages:  1592]\n","Loss: 10.595\n","Test loss: 0.041: 100% 67/67 [00:18\u003c00:00,  3.59it/s]\n","Validation:\n","[Epoch: 41, numImages:   399]\n","Acc:0.9058768767577056, Acc_class:0.8094776304068407, mIoU:0.6980223241881273, fwIoU: 0.8409428170430528\n","Loss: 2.749\n","  0% 0/266 [00:00\u003c?, ?it/s]\n","=\u003eEpoches 42, learning rate = 0.0061,                 previous best = 0.7033\n","Train loss: 0.040: 100% 266/266 [01:40\u003c00:00,  2.65it/s]\n","[Epoch: 42, numImages:  1592]\n","Loss: 10.624\n","Test loss: 0.042: 100% 67/67 [00:17\u003c00:00,  3.79it/s]\n","Validation:\n","[Epoch: 42, numImages:   399]\n","Acc:0.9144553052242148, Acc_class:0.7616840835961631, mIoU:0.689841610064766, fwIoU: 0.8467910542043996\n","Loss: 2.801\n","  0% 0/266 [00:00\u003c?, ?it/s]\n","=\u003eEpoches 43, learning rate = 0.0060,                 previous best = 0.7033\n","Train loss: 0.041: 100% 266/266 [01:42\u003c00:00,  2.61it/s]\n","[Epoch: 43, numImages:  1592]\n","Loss: 10.970\n","Test loss: 0.042: 100% 67/67 [00:17\u003c00:00,  3.78it/s]\n","Validation:\n","[Epoch: 43, numImages:   399]\n","Acc:0.9104694162858709, Acc_class:0.8004694938974732, mIoU:0.7017593892017289, fwIoU: 0.8459453752909367\n","Loss: 2.828\n","  0% 0/266 [00:00\u003c?, ?it/s]\n","=\u003eEpoches 44, learning rate = 0.0059,                 previous best = 0.7033\n","Train loss: 0.040: 100% 266/266 [01:41\u003c00:00,  2.62it/s]\n","[Epoch: 44, numImages:  1592]\n","Loss: 10.626\n","Test loss: 0.041: 100% 67/67 [00:17\u003c00:00,  3.82it/s]\n","Validation:\n","[Epoch: 44, numImages:   399]\n","Acc:0.9059319982411028, Acc_class:0.8334965411549984, mIoU:0.7078198237468649, fwIoU: 0.8430500575939658\n","Loss: 2.771\n","  0% 0/266 [00:00\u003c?, ?it/s]\n","=\u003eEpoches 45, learning rate = 0.0058,                 previous best = 0.7078\n","Train loss: 0.039: 100% 266/266 [01:43\u003c00:00,  2.58it/s]\n","[Epoch: 45, numImages:  1592]\n","Loss: 10.477\n","Test loss: 0.045: 100% 67/67 [00:17\u003c00:00,  3.76it/s]\n","Validation:\n","[Epoch: 45, numImages:   399]\n","Acc:0.9057555294976076, Acc_class:0.8102215215639055, mIoU:0.6981376001017587, fwIoU: 0.8408558413182937\n","Loss: 2.997\n","  0% 0/266 [00:00\u003c?, ?it/s]\n","=\u003eEpoches 46, learning rate = 0.0057,                 previous best = 0.7078\n","Train loss: 0.038: 100% 266/266 [01:41\u003c00:00,  2.63it/s]\n","[Epoch: 46, numImages:  1592]\n","Loss: 10.200\n","Test loss: 0.040: 100% 67/67 [00:18\u003c00:00,  3.58it/s]\n","Validation:\n","[Epoch: 46, numImages:   399]\n","Acc:0.9117083830490925, Acc_class:0.7165360968725274, mIoU:0.6593971190692969, fwIoU: 0.8372312940225654\n","Loss: 2.689\n","  0% 0/266 [00:00\u003c?, ?it/s]\n","=\u003eEpoches 47, learning rate = 0.0056,                 previous best = 0.7078\n","Train loss: 0.039: 100% 266/266 [01:40\u003c00:00,  2.65it/s]\n","[Epoch: 47, numImages:  1592]\n","Loss: 10.398\n","Test loss: 0.039: 100% 67/67 [00:17\u003c00:00,  3.83it/s]\n","Validation:\n","[Epoch: 47, numImages:   399]\n","Acc:0.9146165079452695, Acc_class:0.7756204994354674, mIoU:0.6972106527573831, fwIoU: 0.8486351092182535\n","Loss: 2.609\n","  0% 0/266 [00:00\u003c?, ?it/s]\n","=\u003eEpoches 48, learning rate = 0.0056,                 previous best = 0.7078\n","Train loss: 0.039: 100% 266/266 [01:41\u003c00:00,  2.62it/s]\n","[Epoch: 48, numImages:  1592]\n","Loss: 10.492\n","Test loss: 0.039: 100% 67/67 [00:17\u003c00:00,  3.83it/s]\n","Validation:\n","[Epoch: 48, numImages:   399]\n","Acc:0.915846084628562, Acc_class:0.7745073879337696, mIoU:0.6987843244067669, fwIoU: 0.8500825066297976\n","Loss: 2.645\n","  0% 0/266 [00:00\u003c?, ?it/s]\n","=\u003eEpoches 49, learning rate = 0.0055,                 previous best = 0.7078\n","Train loss: 0.039: 100% 266/266 [01:42\u003c00:00,  2.60it/s]\n","[Epoch: 49, numImages:  1592]\n","Loss: 10.389\n","Test loss: 0.039: 100% 67/67 [00:17\u003c00:00,  3.83it/s]\n","Validation:\n","[Epoch: 49, numImages:   399]\n","Acc:0.9166995533740857, Acc_class:0.7733104019257083, mIoU:0.6996745983814803, fwIoU: 0.8510401377866142\n","Loss: 2.628\n","  0% 0/266 [00:00\u003c?, ?it/s]\n","=\u003eEpoches 50, learning rate = 0.0054,                 previous best = 0.7078\n","Train loss: 0.039: 100% 266/266 [01:40\u003c00:00,  2.64it/s]\n","[Epoch: 50, numImages:  1592]\n","Loss: 10.480\n","Test loss: 0.040: 100% 67/67 [00:18\u003c00:00,  3.59it/s]\n","Validation:\n","[Epoch: 50, numImages:   399]\n","Acc:0.9149232759520406, Acc_class:0.8039437709979531, mIoU:0.7109755164651133, fwIoU: 0.8520221772292569\n","Loss: 2.671\n","  0% 0/266 [00:00\u003c?, ?it/s]\n","=\u003eEpoches 51, learning rate = 0.0053,                 previous best = 0.7110\n","Train loss: 0.039: 100% 266/266 [01:42\u003c00:00,  2.60it/s]\n","[Epoch: 51, numImages:  1592]\n","Loss: 10.307\n","Test loss: 0.040: 100% 67/67 [00:17\u003c00:00,  3.81it/s]\n","Validation:\n","[Epoch: 51, numImages:   399]\n","Acc:0.9145915851874861, Acc_class:0.7440517459478542, mIoU:0.6804695528429999, fwIoU: 0.8447103015895182\n","Loss: 2.647\n","  0% 0/266 [00:00\u003c?, ?it/s]\n","=\u003eEpoches 52, learning rate = 0.0052,                 previous best = 0.7110\n","Train loss: 0.039: 100% 266/266 [01:41\u003c00:00,  2.62it/s]\n","[Epoch: 52, numImages:  1592]\n","Loss: 10.446\n","Test loss: 0.038: 100% 67/67 [00:17\u003c00:00,  3.83it/s]\n","Validation:\n","[Epoch: 52, numImages:   399]\n","Acc:0.9168095773025045, Acc_class:0.7855419750712063, mIoU:0.705860707804464, fwIoU: 0.8525610998391955\n","Loss: 2.541\n","  0% 0/266 [00:00\u003c?, ?it/s]\n","=\u003eEpoches 53, learning rate = 0.0051,                 previous best = 0.7110\n","Train loss: 0.039: 100% 266/266 [01:40\u003c00:00,  2.66it/s]\n","[Epoch: 53, numImages:  1592]\n","Loss: 10.353\n","Test loss: 0.037: 100% 67/67 [00:18\u003c00:00,  3.57it/s]\n","Validation:\n","[Epoch: 53, numImages:   399]\n","Acc:0.9209718778438979, Acc_class:0.7915844519159291, mIoU:0.7162657939515145, fwIoU: 0.8586553839217818\n","Loss: 2.489\n","  0% 0/266 [00:00\u003c?, ?it/s]\n","=\u003eEpoches 54, learning rate = 0.0050,                 previous best = 0.7163\n","Train loss: 0.038: 100% 266/266 [01:41\u003c00:00,  2.61it/s]\n","[Epoch: 54, numImages:  1592]\n","Loss: 10.054\n","Test loss: 0.040: 100% 67/67 [00:17\u003c00:00,  3.83it/s]\n","Validation:\n","[Epoch: 54, numImages:   399]\n","Acc:0.9153589051875344, Acc_class:0.7360500785227755, mIoU:0.6771251780206744, fwIoU: 0.8445726310605418\n","Loss: 2.676\n","  0% 0/266 [00:00\u003c?, ?it/s]\n","=\u003eEpoches 55, learning rate = 0.0049,                 previous best = 0.7163\n","Train loss: 0.038: 100% 266/266 [01:42\u003c00:00,  2.60it/s]\n","[Epoch: 55, numImages:  1592]\n","Loss: 10.181\n","Test loss: 0.039: 100% 67/67 [00:17\u003c00:00,  3.82it/s]\n","Validation:\n","[Epoch: 55, numImages:   399]\n","Acc:0.916152671690588, Acc_class:0.7776907975881553, mIoU:0.7008965896389542, fwIoU: 0.850839762453178\n","Loss: 2.641\n","  0% 0/266 [00:00\u003c?, ?it/s]\n","=\u003eEpoches 56, learning rate = 0.0048,                 previous best = 0.7163\n","Train loss: 0.038: 100% 266/266 [01:40\u003c00:00,  2.64it/s]\n","[Epoch: 56, numImages:  1592]\n","Loss: 10.182\n","Test loss: 0.039: 100% 67/67 [00:18\u003c00:00,  3.68it/s]\n","Validation:\n","[Epoch: 56, numImages:   399]\n","Acc:0.917905931036377, Acc_class:0.765562424963751, mIoU:0.6978270569362781, fwIoU: 0.8516676283546649\n","Loss: 2.585\n","  0% 0/266 [00:00\u003c?, ?it/s]\n","=\u003eEpoches 57, learning rate = 0.0047,                 previous best = 0.7163\n","Train loss: 0.038: 100% 266/266 [01:41\u003c00:00,  2.62it/s]\n","[Epoch: 57, numImages:  1592]\n","Loss: 10.178\n","Test loss: 0.041: 100% 67/67 [00:17\u003c00:00,  3.79it/s]\n","Validation:\n","[Epoch: 57, numImages:   399]\n","Acc:0.9116417382424556, Acc_class:0.7668621861626277, mIoU:0.6877976743123162, fwIoU: 0.8438538516546172\n","Loss: 2.718\n","  0% 0/266 [00:00\u003c?, ?it/s]\n","=\u003eEpoches 58, learning rate = 0.0046,                 previous best = 0.7163\n","Train loss: 0.038: 100% 266/266 [01:41\u003c00:00,  2.61it/s]\n","[Epoch: 58, numImages:  1592]\n","Loss: 10.207\n","Test loss: 0.038: 100% 67/67 [00:17\u003c00:00,  3.83it/s]\n","Validation:\n","[Epoch: 58, numImages:   399]\n","Acc:0.9182602494174745, Acc_class:0.8045011020345123, mIoU:0.7171922127245066, fwIoU: 0.8564333742627255\n","Loss: 2.523\n","  0% 0/266 [00:00\u003c?, ?it/s]\n","=\u003eEpoches 59, learning rate = 0.0045,                 previous best = 0.7172\n","Train loss: 0.037: 100% 266/266 [01:45\u003c00:00,  2.51it/s]\n","[Epoch: 59, numImages:  1592]\n","Loss: 9.749\n","Test loss: 0.039: 100% 67/67 [00:18\u003c00:00,  3.68it/s]\n","Validation:\n","[Epoch: 59, numImages:   399]\n","Acc:0.9156757775298073, Acc_class:0.7444971039789736, mIoU:0.6825319940951893, fwIoU: 0.8461307173081649\n","Loss: 2.620\n","  0% 0/266 [00:00\u003c?, ?it/s]\n","=\u003eEpoches 60, learning rate = 0.0044,                 previous best = 0.7172\n","Train loss: 0.037: 100% 266/266 [01:42\u003c00:00,  2.59it/s]\n","[Epoch: 60, numImages:  1592]\n","Loss: 9.918\n","Test loss: 0.039: 100% 67/67 [00:18\u003c00:00,  3.60it/s]\n","Validation:\n","[Epoch: 60, numImages:   399]\n","Acc:0.9167338090713524, Acc_class:0.8078307834895382, mIoU:0.7159001074069729, fwIoU: 0.8547613904697388\n","Loss: 2.625\n","  0% 0/266 [00:00\u003c?, ?it/s]\n","=\u003eEpoches 61, learning rate = 0.0043,                 previous best = 0.7172\n","Train loss: 0.037: 100% 266/266 [01:45\u003c00:00,  2.51it/s]\n","[Epoch: 61, numImages:  1592]\n","Loss: 9.877\n","Test loss: 0.041: 100% 67/67 [00:18\u003c00:00,  3.55it/s]\n","Validation:\n","[Epoch: 61, numImages:   399]\n","Acc:0.9097041723886871, Acc_class:0.7023451643169689, mIoU:0.6471765522200429, fwIoU: 0.8326055128810836\n","Loss: 2.714\n","  0% 0/266 [00:00\u003c?, ?it/s]\n","=\u003eEpoches 62, learning rate = 0.0042,                 previous best = 0.7172\n","Train loss: 0.037: 100% 266/266 [01:45\u003c00:00,  2.51it/s]\n","[Epoch: 62, numImages:  1592]\n","Loss: 9.852\n","Test loss: 0.040: 100% 67/67 [00:18\u003c00:00,  3.68it/s]\n","Validation:\n","[Epoch: 62, numImages:   399]\n","Acc:0.9100287491677375, Acc_class:0.7299903126730924, mIoU:0.6650143239349009, fwIoU: 0.837147721811721\n","Loss: 2.692\n","  0% 0/266 [00:00\u003c?, ?it/s]\n","=\u003eEpoches 63, learning rate = 0.0041,                 previous best = 0.7172\n","Train loss: 0.037: 100% 266/266 [01:43\u003c00:00,  2.57it/s]\n","[Epoch: 63, numImages:  1592]\n","Loss: 9.801\n","Test loss: 0.038: 100% 67/67 [00:19\u003c00:00,  3.53it/s]\n","Validation:\n","[Epoch: 63, numImages:   399]\n","Acc:0.9174488074698486, Acc_class:0.8070589279279061, mIoU:0.716849905554451, fwIoU: 0.855622151292998\n","Loss: 2.553\n","  0% 0/266 [00:00\u003c?, ?it/s]\n","=\u003eEpoches 64, learning rate = 0.0040,                 previous best = 0.7172\n","Train loss: 0.038: 100% 266/266 [01:42\u003c00:00,  2.58it/s]\n","[Epoch: 64, numImages:  1592]\n","Loss: 10.034\n","Test loss: 0.039: 100% 67/67 [00:17\u003c00:00,  3.75it/s]\n","Validation:\n","[Epoch: 64, numImages:   399]\n","Acc:0.9165505596616204, Acc_class:0.7621713008836075, mIoU:0.6936845123949666, fwIoU: 0.8495154345064334\n","Loss: 2.587\n","  0% 0/266 [00:00\u003c?, ?it/s]\n","=\u003eEpoches 65, learning rate = 0.0039,                 previous best = 0.7172\n","Train loss: 0.037: 100% 266/266 [01:44\u003c00:00,  2.55it/s]\n","[Epoch: 65, numImages:  1592]\n","Loss: 9.846\n","Test loss: 0.040: 100% 67/67 [00:18\u003c00:00,  3.71it/s]\n","Validation:\n","[Epoch: 65, numImages:   399]\n","Acc:0.9171620195722978, Acc_class:0.7689070308040573, mIoU:0.6982497964513376, fwIoU: 0.8511155044625498\n","Loss: 2.663\n","  0% 0/266 [00:00\u003c?, ?it/s]\n","=\u003eEpoches 66, learning rate = 0.0038,                 previous best = 0.7172\n","Train loss: 0.038: 100% 266/266 [01:42\u003c00:00,  2.59it/s]\n","[Epoch: 66, numImages:  1592]\n","Loss: 10.014\n","Test loss: 0.039: 100% 67/67 [00:19\u003c00:00,  3.49it/s]\n","Validation:\n","[Epoch: 66, numImages:   399]\n","Acc:0.9176130862515697, Acc_class:0.7522197190126648, mIoU:0.6901564358370146, fwIoU: 0.8496067076937345\n","Loss: 2.583\n","  0% 0/266 [00:00\u003c?, ?it/s]\n","=\u003eEpoches 67, learning rate = 0.0037,                 previous best = 0.7172\n","Train loss: 0.037: 100% 266/266 [01:42\u003c00:00,  2.60it/s]\n","[Epoch: 67, numImages:  1592]\n","Loss: 9.799\n","Test loss: 0.038: 100% 67/67 [00:17\u003c00:00,  3.74it/s]\n","Validation:\n","[Epoch: 67, numImages:   399]\n","Acc:0.9190495303955316, Acc_class:0.8073125222258176, mIoU:0.719874281974505, fwIoU: 0.8577509016333709\n","Loss: 2.575\n","  0% 0/266 [00:00\u003c?, ?it/s]\n","=\u003eEpoches 68, learning rate = 0.0036,                 previous best = 0.7199\n","Train loss: 0.037: 100% 266/266 [01:46\u003c00:00,  2.50it/s]\n","[Epoch: 68, numImages:  1592]\n","Loss: 9.733\n","Test loss: 0.038: 100% 67/67 [00:17\u003c00:00,  3.73it/s]\n","Validation:\n","[Epoch: 68, numImages:   399]\n","Acc:0.9182099182081183, Acc_class:0.759696253813468, mIoU:0.695261888721161, fwIoU: 0.8513298256331624\n","Loss: 2.559\n","  0% 0/266 [00:00\u003c?, ?it/s]\n","=\u003eEpoches 69, learning rate = 0.0035,                 previous best = 0.7199\n","Train loss: 0.037: 100% 266/266 [01:42\u003c00:00,  2.59it/s]\n","[Epoch: 69, numImages:  1592]\n","Loss: 9.949\n","Test loss: 0.039: 100% 67/67 [00:19\u003c00:00,  3.44it/s]\n","Validation:\n","[Epoch: 69, numImages:   399]\n","Acc:0.9178365720585638, Acc_class:0.74726270052443, mIoU:0.6877737400703657, fwIoU: 0.8492334583573705\n","Loss: 2.629\n","  0% 0/266 [00:00\u003c?, ?it/s]\n","=\u003eEpoches 70, learning rate = 0.0034,                 previous best = 0.7199\n","Train loss: 0.035: 100% 266/266 [01:42\u003c00:00,  2.59it/s]\n","[Epoch: 70, numImages:  1592]\n","Loss: 9.391\n","Test loss: 0.038: 100% 67/67 [00:19\u003c00:00,  3.49it/s]\n","Validation:\n","[Epoch: 70, numImages:   399]\n","Acc:0.9183053141824081, Acc_class:0.7758101840557159, mIoU:0.7037653635224251, fwIoU: 0.853402350216027\n","Loss: 2.521\n","  0% 0/266 [00:00\u003c?, ?it/s]\n","=\u003eEpoches 71, learning rate = 0.0033,                 previous best = 0.7199\n","Train loss: 0.035: 100% 266/266 [01:41\u003c00:00,  2.61it/s]\n","[Epoch: 71, numImages:  1592]\n","Loss: 9.272\n","Test loss: 0.039: 100% 67/67 [00:17\u003c00:00,  3.76it/s]\n","Validation:\n","[Epoch: 71, numImages:   399]\n","Acc:0.9160077539965908, Acc_class:0.7776143015292678, mIoU:0.7006056662874822, fwIoU: 0.8506447189440566\n","Loss: 2.580\n","  0% 0/266 [00:00\u003c?, ?it/s]\n","=\u003eEpoches 72, learning rate = 0.0032,                 previous best = 0.7199\n","Train loss: 0.035: 100% 266/266 [01:42\u003c00:00,  2.59it/s]\n","[Epoch: 72, numImages:  1592]\n","Loss: 9.314\n","Test loss: 0.037: 100% 67/67 [00:17\u003c00:00,  3.75it/s]\n","Validation:\n","[Epoch: 72, numImages:   399]\n","Acc:0.9218541358507052, Acc_class:0.796166821343794, mIoU:0.7200483235411288, fwIoU: 0.8603133398277891\n","Loss: 2.448\n","  0% 0/266 [00:00\u003c?, ?it/s]\n","=\u003eEpoches 73, learning rate = 0.0031,                 previous best = 0.7200\n","Train loss: 0.036: 100% 266/266 [01:45\u003c00:00,  2.53it/s]\n","[Epoch: 73, numImages:  1592]\n","Loss: 9.580\n","Test loss: 0.037: 100% 67/67 [00:18\u003c00:00,  3.72it/s]\n","Validation:\n","[Epoch: 73, numImages:   399]\n","Acc:0.9221808839666966, Acc_class:0.7966274789147576, mIoU:0.7208747448705984, fwIoU: 0.8607955232780153\n","Loss: 2.501\n","  0% 0/266 [00:00\u003c?, ?it/s]\n","=\u003eEpoches 74, learning rate = 0.0030,                 previous best = 0.7209\n","Train loss: 0.036: 100% 266/266 [01:43\u003c00:00,  2.56it/s]\n","[Epoch: 74, numImages:  1592]\n","Loss: 9.483\n","Test loss: 0.037: 100% 67/67 [00:19\u003c00:00,  3.51it/s]\n","Validation:\n","[Epoch: 74, numImages:   399]\n","Acc:0.9185596748769583, Acc_class:0.8238368567991194, mIoU:0.7259964216755037, fwIoU: 0.858667496186859\n","Loss: 2.506\n","  0% 0/266 [00:00\u003c?, ?it/s]\n","=\u003eEpoches 75, learning rate = 0.0029,                 previous best = 0.7260\n","Train loss: 0.036: 100% 266/266 [01:44\u003c00:00,  2.55it/s]\n","[Epoch: 75, numImages:  1592]\n","Loss: 9.611\n","Test loss: 0.037: 100% 67/67 [00:19\u003c00:00,  3.49it/s]\n","Validation:\n","[Epoch: 75, numImages:   399]\n","Acc:0.9209678684892831, Acc_class:0.8029654514168212, mIoU:0.721490389933668, fwIoU: 0.859848297057924\n","Loss: 2.487\n","  0% 0/266 [00:00\u003c?, ?it/s]\n","=\u003eEpoches 76, learning rate = 0.0028,                 previous best = 0.7260\n","Train loss: 0.036: 100% 266/266 [01:42\u003c00:00,  2.58it/s]\n","[Epoch: 76, numImages:  1592]\n","Loss: 9.528\n","Test loss: 0.037: 100% 67/67 [00:18\u003c00:00,  3.72it/s]\n","Validation:\n","[Epoch: 76, numImages:   399]\n","Acc:0.9202042530948051, Acc_class:0.7800361491398426, mIoU:0.7092888720108863, fwIoU: 0.8563598477808627\n","Loss: 2.472\n","  0% 0/266 [00:00\u003c?, ?it/s]\n","=\u003eEpoches 77, learning rate = 0.0027,                 previous best = 0.7260\n","Train loss: 0.036: 100% 266/266 [01:43\u003c00:00,  2.57it/s]\n","[Epoch: 77, numImages:  1592]\n","Loss: 9.473\n","Test loss: 0.037: 100% 67/67 [00:17\u003c00:00,  3.73it/s]\n","Validation:\n","[Epoch: 77, numImages:   399]\n","Acc:0.9197470628644233, Acc_class:0.7783099027874332, mIoU:0.7076030908904987, fwIoU: 0.85556454111159\n","Loss: 2.473\n","  0% 0/266 [00:00\u003c?, ?it/s]\n","=\u003eEpoches 78, learning rate = 0.0026,                 previous best = 0.7260\n","Train loss: 0.035: 100% 266/266 [01:43\u003c00:00,  2.56it/s]\n","[Epoch: 78, numImages:  1592]\n","Loss: 9.420\n","Test loss: 0.037: 100% 67/67 [00:18\u003c00:00,  3.72it/s]\n","Validation:\n","[Epoch: 78, numImages:   399]\n","Acc:0.9213869174720827, Acc_class:0.8088134511607397, mIoU:0.7248695260186384, fwIoU: 0.8609936468442462\n","Loss: 2.461\n","  0% 0/266 [00:00\u003c?, ?it/s]\n","=\u003eEpoches 79, learning rate = 0.0025,                 previous best = 0.7260\n","Train loss: 0.036: 100% 266/266 [01:43\u003c00:00,  2.56it/s]\n","[Epoch: 79, numImages:  1592]\n","Loss: 9.657\n","Test loss: 0.038: 100% 67/67 [00:19\u003c00:00,  3.52it/s]\n","Validation:\n","[Epoch: 79, numImages:   399]\n","Acc:0.9189259356112315, Acc_class:0.7789574132396009, mIoU:0.7064405302397744, fwIoU: 0.8545701788471984\n","Loss: 2.516\n","  0% 0/266 [00:00\u003c?, ?it/s]\n","=\u003eEpoches 80, learning rate = 0.0023,                 previous best = 0.7260\n","Train loss: 0.035: 100% 266/266 [01:41\u003c00:00,  2.62it/s]\n","[Epoch: 80, numImages:  1592]\n","Loss: 9.263\n","Test loss: 0.037: 100% 67/67 [00:17\u003c00:00,  3.73it/s]\n","Validation:\n","[Epoch: 80, numImages:   399]\n","Acc:0.9197476818859196, Acc_class:0.7643045349446941, mIoU:0.7004379685367975, fwIoU: 0.8538872582351394\n","Loss: 2.498\n","  0% 0/266 [00:00\u003c?, ?it/s]\n","=\u003eEpoches 81, learning rate = 0.0022,                 previous best = 0.7260\n","Train loss: 0.035: 100% 266/266 [01:43\u003c00:00,  2.57it/s]\n","[Epoch: 81, numImages:  1592]\n","Loss: 9.317\n","Test loss: 0.036: 100% 67/67 [00:17\u003c00:00,  3.74it/s]\n","Validation:\n","[Epoch: 81, numImages:   399]\n","Acc:0.9225911237974329, Acc_class:0.7880152594906454, mIoU:0.717590984483486, fwIoU: 0.86040145866672\n","Loss: 2.397\n","  0% 0/266 [00:00\u003c?, ?it/s]\n","=\u003eEpoches 82, learning rate = 0.0021,                 previous best = 0.7260\n","Train loss: 0.034: 100% 266/266 [01:40\u003c00:00,  2.64it/s]\n","[Epoch: 82, numImages:  1592]\n","Loss: 9.162\n","Test loss: 0.035: 100% 67/67 [00:19\u003c00:00,  3.49it/s]\n","Validation:\n","[Epoch: 82, numImages:   399]\n","Acc:0.9239857506584651, Acc_class:0.7958960426065023, mIoU:0.7239605477791201, fwIoU: 0.8631195390033228\n","Loss: 2.372\n","  0% 0/266 [00:00\u003c?, ?it/s]\n","=\u003eEpoches 83, learning rate = 0.0020,                 previous best = 0.7260\n","Train loss: 0.036: 100% 266/266 [01:42\u003c00:00,  2.61it/s]\n","[Epoch: 83, numImages:  1592]\n","Loss: 9.606\n","Test loss: 0.037: 100% 67/67 [00:17\u003c00:00,  3.73it/s]\n","Validation:\n","[Epoch: 83, numImages:   399]\n","Acc:0.9201048096722699, Acc_class:0.7684140164850426, mIoU:0.7032294860099964, fwIoU: 0.8548558646910345\n","Loss: 2.492\n","  0% 0/266 [00:00\u003c?, ?it/s]\n","=\u003eEpoches 84, learning rate = 0.0019,                 previous best = 0.7260\n","Train loss: 0.035: 100% 266/266 [01:44\u003c00:00,  2.55it/s]\n","[Epoch: 84, numImages:  1592]\n","Loss: 9.407\n","Test loss: 0.037: 100% 67/67 [00:17\u003c00:00,  3.73it/s]\n","Validation:\n","[Epoch: 84, numImages:   399]\n","Acc:0.9210762353447732, Acc_class:0.7788682075141322, mIoU:0.7103075362862155, fwIoU: 0.8573660742181719\n","Loss: 2.484\n","  0% 0/266 [00:00\u003c?, ?it/s]\n","=\u003eEpoches 85, learning rate = 0.0018,                 previous best = 0.7260\n","Train loss: 0.035: 100% 266/266 [01:43\u003c00:00,  2.57it/s]\n","[Epoch: 85, numImages:  1592]\n","Loss: 9.376\n","Test loss: 0.038: 100% 67/67 [00:17\u003c00:00,  3.80it/s]\n","Validation:\n","[Epoch: 85, numImages:   399]\n","Acc:0.9207227074065093, Acc_class:0.778700435839142, mIoU:0.7095752787722059, fwIoU: 0.8568837559433836\n","Loss: 2.515\n","  0% 0/266 [00:00\u003c?, ?it/s]\n","=\u003eEpoches 86, learning rate = 0.0017,                 previous best = 0.7260\n","Train loss: 0.035: 100% 266/266 [01:43\u003c00:00,  2.57it/s]\n","[Epoch: 86, numImages:  1592]\n","Loss: 9.254\n","Test loss: 0.037: 100% 67/67 [00:17\u003c00:00,  3.78it/s]\n","Validation:\n","[Epoch: 86, numImages:   399]\n","Acc:0.9202900970912361, Acc_class:0.7655837524116726, mIoU:0.7020865153244352, fwIoU: 0.8547484686577178\n","Loss: 2.468\n","  0% 0/266 [00:00\u003c?, ?it/s]\n","=\u003eEpoches 87, learning rate = 0.0016,                 previous best = 0.7260\n","Train loss: 0.034: 100% 266/266 [01:43\u003c00:00,  2.58it/s]\n","[Epoch: 87, numImages:  1592]\n","Loss: 8.986\n","Test loss: 0.036: 100% 67/67 [00:17\u003c00:00,  3.74it/s]\n","Validation:\n","[Epoch: 87, numImages:   399]\n","Acc:0.9224621197175955, Acc_class:0.8044684844014551, mIoU:0.7249712233634601, fwIoU: 0.8619868742224984\n","Loss: 2.395\n","  0% 0/266 [00:00\u003c?, ?it/s]\n","=\u003eEpoches 88, learning rate = 0.0015,                 previous best = 0.7260\n","Train loss: 0.035: 100% 266/266 [01:43\u003c00:00,  2.58it/s]\n","[Epoch: 88, numImages:  1592]\n","Loss: 9.283\n","Test loss: 0.037: 100% 67/67 [00:17\u003c00:00,  3.78it/s]\n","Validation:\n","[Epoch: 88, numImages:   399]\n","Acc:0.9213870698466049, Acc_class:0.7610691875806601, mIoU:0.7016728633574025, fwIoU: 0.855604251324205\n","Loss: 2.472\n","  0% 0/266 [00:00\u003c?, ?it/s]\n","=\u003eEpoches 89, learning rate = 0.0014,                 previous best = 0.7260\n","Train loss: 0.034: 100% 266/266 [01:43\u003c00:00,  2.57it/s]\n","[Epoch: 89, numImages:  1592]\n","Loss: 9.151\n","Test loss: 0.036: 100% 67/67 [00:17\u003c00:00,  3.73it/s]\n","Validation:\n","[Epoch: 89, numImages:   399]\n","Acc:0.9222118445649212, Acc_class:0.8031403399234776, mIoU:0.723902285839988, fwIoU: 0.8615175142937472\n","Loss: 2.416\n","  0% 0/266 [00:00\u003c?, ?it/s]\n","=\u003eEpoches 90, learning rate = 0.0013,                 previous best = 0.7260\n","Train loss: 0.034: 100% 266/266 [01:42\u003c00:00,  2.60it/s]\n","[Epoch: 90, numImages:  1592]\n","Loss: 8.928\n","Test loss: 0.035: 100% 67/67 [00:17\u003c00:00,  3.75it/s]\n","Validation:\n","[Epoch: 90, numImages:   399]\n","Acc:0.92311520644305, Acc_class:0.8014596614043225, mIoU:0.7248550833361947, fwIoU: 0.8625471148746188\n","Loss: 2.348\n","  0% 0/266 [00:00\u003c?, ?it/s]\n","=\u003eEpoches 91, learning rate = 0.0011,                 previous best = 0.7260\n","Train loss: 0.034: 100% 266/266 [01:42\u003c00:00,  2.60it/s]\n","[Epoch: 91, numImages:  1592]\n","Loss: 8.956\n","Test loss: 0.035: 100% 67/67 [00:19\u003c00:00,  3.53it/s]\n","Validation:\n","[Epoch: 91, numImages:   399]\n","Acc:0.9234656107036092, Acc_class:0.8107904262939898, mIoU:0.7296781373281501, fwIoU: 0.8639653799456968\n","Loss: 2.351\n","  0% 0/266 [00:00\u003c?, ?it/s]\n","=\u003eEpoches 92, learning rate = 0.0010,                 previous best = 0.7297\n","Train loss: 0.035: 100% 266/266 [01:44\u003c00:00,  2.55it/s]\n","[Epoch: 92, numImages:  1592]\n","Loss: 9.357\n","Test loss: 0.035: 100% 67/67 [00:19\u003c00:00,  3.51it/s]\n","Validation:\n","[Epoch: 92, numImages:   399]\n","Acc:0.9240966126467558, Acc_class:0.8044893856512594, mIoU:0.7281005751073018, fwIoU: 0.8641735406867186\n","Loss: 2.372\n","  0% 0/266 [00:00\u003c?, ?it/s]\n","=\u003eEpoches 93, learning rate = 0.0009,                 previous best = 0.7297\n","Train loss: 0.035: 100% 266/266 [01:41\u003c00:00,  2.61it/s]\n","[Epoch: 93, numImages:  1592]\n","Loss: 9.287\n","Test loss: 0.036: 100% 67/67 [00:19\u003c00:00,  3.50it/s]\n","Validation:\n","[Epoch: 93, numImages:   399]\n","Acc:0.9212405046030867, Acc_class:0.7710077023568636, mIoU:0.7066338979969231, fwIoU: 0.8566494073567916\n","Loss: 2.417\n","  0% 0/266 [00:00\u003c?, ?it/s]\n","=\u003eEpoches 94, learning rate = 0.0008,                 previous best = 0.7297\n","Train loss: 0.033: 100% 266/266 [01:42\u003c00:00,  2.58it/s]\n","[Epoch: 94, numImages:  1592]\n","Loss: 8.764\n","Test loss: 0.035: 100% 67/67 [00:17\u003c00:00,  3.73it/s]\n","Validation:\n","[Epoch: 94, numImages:   399]\n","Acc:0.9236346130955179, Acc_class:0.7948623877152268, mIoU:0.7228076918373976, fwIoU: 0.8625394632490594\n","Loss: 2.333\n","  0% 0/266 [00:00\u003c?, ?it/s]\n","=\u003eEpoches 95, learning rate = 0.0007,                 previous best = 0.7297\n","Train loss: 0.033: 100% 266/266 [01:44\u003c00:00,  2.54it/s]\n","[Epoch: 95, numImages:  1592]\n","Loss: 8.893\n","Test loss: 0.035: 100% 67/67 [00:18\u003c00:00,  3.68it/s]\n","Validation:\n","[Epoch: 95, numImages:   399]\n","Acc:0.9223502196778725, Acc_class:0.7922856468773439, mIoU:0.7191678157628933, fwIoU: 0.8605529027062376\n","Loss: 2.347\n","  0% 0/266 [00:00\u003c?, ?it/s]\n","=\u003eEpoches 96, learning rate = 0.0006,                 previous best = 0.7297\n","Train loss: 0.033: 100% 266/266 [01:44\u003c00:00,  2.55it/s]\n","[Epoch: 96, numImages:  1592]\n","Loss: 8.767\n","Test loss: 0.035: 100% 67/67 [00:18\u003c00:00,  3.70it/s]\n","Validation:\n","[Epoch: 96, numImages:   399]\n","Acc:0.9235878150703946, Acc_class:0.8017206854678932, mIoU:0.7258751061045119, fwIoU: 0.8632054045537509\n","Loss: 2.331\n","  0% 0/266 [00:00\u003c?, ?it/s]\n","=\u003eEpoches 97, learning rate = 0.0004,                 previous best = 0.7297\n","Train loss: 0.032: 100% 266/266 [01:43\u003c00:00,  2.56it/s]\n","[Epoch: 97, numImages:  1592]\n","Loss: 8.492\n","Test loss: 0.035: 100% 67/67 [00:18\u003c00:00,  3.70it/s]\n","Validation:\n","[Epoch: 97, numImages:   399]\n","Acc:0.9226259223289348, Acc_class:0.7839049811353598, mIoU:0.7156668646057143, fwIoU: 0.8599841804126983\n","Loss: 2.352\n","  0% 0/266 [00:00\u003c?, ?it/s]\n","=\u003eEpoches 98, learning rate = 0.0003,                 previous best = 0.7297\n","Train loss: 0.032: 100% 266/266 [01:43\u003c00:00,  2.57it/s]\n","[Epoch: 98, numImages:  1592]\n","Loss: 8.584\n","Test loss: 0.035: 100% 67/67 [00:17\u003c00:00,  3.74it/s]\n","Validation:\n","[Epoch: 98, numImages:   399]\n","Acc:0.9241217925365455, Acc_class:0.7988529850519269, mIoU:0.7255889193794707, fwIoU: 0.8636176500796744\n","Loss: 2.334\n","  0% 0/266 [00:00\u003c?, ?it/s]\n","=\u003eEpoches 99, learning rate = 0.0002,                 previous best = 0.7297\n","Train loss: 0.032: 100% 266/266 [01:41\u003c00:00,  2.62it/s]\n","[Epoch: 99, numImages:  1592]\n","Loss: 8.481\n","Test loss: 0.035: 100% 67/67 [00:18\u003c00:00,  3.54it/s]\n","Validation:\n","[Epoch: 99, numImages:   399]\n","Acc:0.9224822998183763, Acc_class:0.7996582938405588, mIoU:0.7228350946102731, fwIoU: 0.8615158530655178\n","Loss: 2.351\n"]}],"source":["!python train.py --dataset corrosion --backbone resnet --lr 0.01 --workers 1 --epochs 100 --batch-size 6 --gpu-ids 0 --checkname train_corrosion_no_ft_04"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyM1hoaUL6ul+i9cnUEtbnQL","collapsed_sections":[],"mount_file_id":"1QcvPW4mqBHCuyTBIp6NSx3ao7lrg_RKs","name":"Train_deeplab.ipynb","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}